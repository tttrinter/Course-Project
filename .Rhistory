folds$train[[1]]
modelFit=train(type~.,data=training,method="glm")
args(train.default)
function(x,y,method="rf",preProcess=NULL,...,weights=NULL,
metric=ifelse(is.factor(y),"Accuracy","RMSE"),
maximize=ifelse(metric=="RMSE",FALSE,TRUE),
trControl=trainControl(),tuneGrid=NULL,tuneLength=3)
)
args(train.default)
args(trainControl)
install.packages("ISLR")
library(ISLR); library(ggplot2);library(caret)
data(Wage)
summary(Wage)
inTrain=createDataPartition(y=Wage$wage,p=0.7,list=FALSE)
training=Wage[inTrain,]
testing=Wage[-inTrain,]
dim(training);dim(testing)
featurePlot(x=training[,c("age","education","jobclass")],
y=training$wage,
plot="pairs")
qplot(age,wage,data=training)
qplot(age,wage,color=jobclass,data=training )
qq=qplot(age,wage,color=education,data=training )
qq + geom_smooth(method="lm",formula=y~x)
install.packages("Hmisc")
library(hmisc)
cutWage=cut2(training$wage,g=3)
table(cutWage)
library(Hmisc)
cutWage=cut2(training$wage,g=3)
table(cutWage)
p1=qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot"))
p1
p2=qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot","jitter"))
grid.arrange(p1,p2,ncol=2)
install.packages("gridExtra")
grid.arrange(p1,p2,ncol=2)
library(gridExtra)
grid.arrange(p1,p2,ncol=2)
library(ggplot2)
grid.arrange(p1,p2,ncol=2)
t1=table(cutWage,training$jobclass)
t1
prop.table(t1,1)
qplot(wage,color=education, data=training, geom="density")
library(caret);library(kernlab);data(spam)
inTrain=createDataPartition(y=spam$type,p=0.75,list=FALSE)
training=spam[inTrain,]
testin=spam[-inTrain,]
hist(training$capitalAve,main="",xlab=ave. "capital run length")
hist(training$capitalAve,main="",xlab="ave capital run length")
mean(training$capitalAve)
sd(training$capitalAve)
trainCapAve=training$capitalAve
trainCapAveS=(trainCapAve-mean(trainCapAve))/sd(trainCapAve)
mean(trainCapAveS); sd(trainCapAveS)
testCapAve=testing$capitalAve
testCapAveS=(testCapAve-mean(trainCapAve))/sd(trainCapAve)
mean(testCapAveS); sd(testCapAveS)
rm(testin)
testing=spam[-inTrain,]
testCapAve=testing$capitalAve
testCapAveS=(testCapAve-mean(trainCapAve))/sd(trainCapAve)
mean(testCapAveS); sd(testCapAveS)
preObj=preProcess(training[,-58],method=c("center","scale"))
trainCapAveS=predict(preObj,training[,-58])$capitalAve
meantrainCapAveS
mean(trainCapAveS)
preObj=preProcess(training[,-58],method=c("center","scale"))
trainCapAveS=predict(preObj,training[,-58])$capitalAve
mean(trainCapAveS)
mean(trainCapAveS);sd(trainCapAveS)
testCapAveS=predict(preObj,testing[,-58])$capitalAve
mean(testCapAveS);sd(testCapAveS)
set.seed(32343)
modelFit=train(type ~., data=training,
preProcess=c("center","scale"), method="glm")
modelFit
)
modelFit=train(type ~., data=training,
preProcess=c("center","scale"), method="glm")
preObj=preProcess(training[,-58],method=c("BoxCox"))
trainCapAveS=predict(preObj,training[,-58])$capitalAve
par(mfrow=c(1,2)); hist(trainCapAveS); qqnorm(trainCapAveS)
training$capAve=training$capitalAve
selectNA=rbinom(dim(training)[1],size=1,prob=0.5)==1
training$capAve[selectNA]=NA
View(training)
preObj=preProcess(training[,-58],method="knnImpute")
capAve=predict(preObj,training[,-58])$capAve
install.packages("RANN")
library(RANN)
capAve=predict(preObj,training[,-58])$capAve
capAveTruth=training$capitalAve
capAveTruth=(capAveTruth-meancapAveTruth))/sd(capAveTruth)
capAveTruth=(capAveTruth-mean(capAveTruth))/sd(capAveTruth)
quantile(capAve-capAveTruth)
quantile((capAve-capAveTruth[selectNA])
)
quantile((capAve-capAveTruth)[selectNA])
quantile(capAve-capAveTruth)[!selectNA]
quantile((capAve-capAveTruth)[!selectNA])
library(kernlab); data(spam)
spam$capitalAveSq=spam$capitalAve^2
library(ISLR);library(caret);data(Wage)
inTrain = createDataPartition(y=Wage$wage,p=0.7,list=FALSE)
training=Wage[inTrain,]; testing=Wage[-inTrain,]
table(training$jobclass)
dummies=dummyVars(wage ~ jobclass, data=training)
head(predict(dummies,newdata=training))
nsv=nearZeroVar(training,saveMetrics=TRUE)
nsv
library(splines)
bsBasis=bs(training$age,df=3)
bsBasis
lm1=lm(wage ~ bsBasis, data=training)
plot(training$age,training$wage, pch=19,cex=0.5)
points(training$age,predict(lm1,newdata=training),col="red",pch=19,cex=0.5)
predict(bsBasis,age=testing$age)
M=abs(cor(training[,-58]))
diag(M)=0
which(M>0.8,arr.ind=T)
data(spam)
inTrain=createDataPartition(y=spam$type,p=0.75,list=FALSE)
training=spam[inTrain,]
testing=spam[-inTrain,]
#Leave out the outcome and look at the correlation of predictor values
M=abs(cor(training[,-58]))
diag(M)=0
which(M>0.8,arr.ind=T)
which(M>0.8,arr.ind=T)
names(spam)[c(34,32)]
plot(spam[,34],spam[,32])
smallSpam=spam[,c(34,32)]
prComp=prcomp(smallSpam)
plot(prComp$x[,1],prComp$x[,2])
prComp$rotation
typeColor=((spam$type=="spam")*1+1)
prComp=prcomp(log10(spam[,-58]+1))
plot(prComp$x[,1],prComp$x[,2],col=typeColor,xlab="PC1",ylab="PC2")
prComp
preProc=preProcess(log10(spam[,-58]+1),method="pca",pcaComp=2)
spamPC=predict(preProc,log10(spam[,-58]+1))
plot(spamPC[,1],spamPC[,2],col=typeColor)
trainPC=predict(preProc,log10(training[,-58]+1))
modelFit=train(training$type ~ .,method="glm",data=trainPC)
testPC=predict(preProc,log10(testing[,-58]+1))
confusionMatrix(testing$type,predict(modelFit,testPC))
modelFit=train(training$type ~ .,method="glm",preProcess="pca",data=training)
warnings()
confusionMatrix(testing$type,predict(modelFit,testing))
library(caret); data(faithful);set.seed(333)
inTrain=createDataPartition(y=faithful$waiting,p=0.5,list=FALSE)
trainFaith=faithful[inTrain,]; testFaith=faithful[-inTrain,]
head(trainFaith)
plot(trainFaith$waiting, traiFaith$eruptions, pch=19,col="green",xlab="waiting",ylab="duration")
plot(trainFaith$waiting, trainFaith$eruptions, pch=19,col="green",xlab="waiting",ylab="duration")
lm1=lm(eruptions ~ waiting, data=trainFaith)
summary(lm1)
lines(trainFaith$waiting,lm1$fitted,lwd=3)
lines(trainFaith$waiting,lm1$fitted,lwd=2)
plot(trainFaith$waiting, trainFaith$eruptions, pch=19,col="green",xlab="waiting",ylab="duration")
lines(trainFaith$waiting,lm1$fitted,lwd=2)
coef(lm1)[1]+coef(lm1)[2]*80
newdata=data.frame(waiting=80)
predict(lm1,newdata)
newdata=data.frame(waiting=c(10,20,30,40,50,60,70,80))
predict(lm1,newdata)
par(mfrow=c(1,2))
plot(trainFaith$waiting,trainFaith$eruptions, pch=19,col="green",xlab="waiting",ylab="duration")
lines(trainFaith$waiting,lm1$fitted,lwd=2)
plot(testFaith$waiting,testFaith$eruptions, pch=19,col="green",xlab="waiting",ylab="duration")
lines(testFaith$waiting,lm1$fitted,lwd=2)
plot(trainFaith$waiting,trainFaith$eruptions, pch=19,col="green",xlab="waiting",ylab="duration")
lines(trainFaith$waiting,lm1$fitted,lwd=2)
plot(testFaith$waiting,testFaith$eruptions, pch=19,col="green",xlab="waiting",ylab="duration")
lines(testFaith$waiting,predict(lm1,newdata=testFaith),lwd=2)
par(mfrow=c(1,2))
plot(trainFaith$waiting,trainFaith$eruptions, pch=19,col="green",xlab="waiting",ylab="duration")
lines(trainFaith$waiting,lm1$fitted,lwd=2)
plot(testFaith$waiting,testFaith$eruptions, pch=19,col="green",xlab="waiting",ylab="duration")
lines(testFaith$waiting,predict(lm1,newdata=testFaith),lwd=2)
#Predictions vs. Test
par(mfrow=c(1,2))
plot(trainFaith$waiting,trainFaith$eruptions, pch=19,col="green",xlab="waiting",ylab="duration", main="Training")
lines(trainFaith$waiting,lm1$fitted,lwd=2)
plot(testFaith$waiting,testFaith$eruptions, pch=19,col="green",xlab="waiting",ylab="duration", main="Test")
lines(testFaith$waiting,predict(lm1,newdata=testFaith),lwd=2)
sqrt(sum((lm1$fitted-trainFaith$eruptions)^2))
sqrt(sum((predict(lm1,newdata=testFaith)-testFaith$eruptions)^2))
pred1=predict(lm1,newdata=testFaith,interval="prediction")
ord=order(testFaith$waiting)
plot(testFaith$waiting,testFaith$eruptions, pch=19,col="blue")
matlines(testFaith$waiting[ord],pred1[ord,],type="l",,col=c(1,2,2),lty=c(1,1,1),lwd=2)
par(mfrow=c(1,1))
plot(testFaith$waiting,testFaith$eruptions, pch=19,col="blue")
matlines(testFaith$waiting[ord],pred1[ord,],type="l",,col=c(1,2,2),lty=c(1,1,1),lwd=2)
modFit=train(eruptions ~ waiting, data=trainFaith, method="lm")
summary(modFit$finalModel)
library(ISLR);library(ggplot2);library(caret); data(Wage)
Wage=subset(Wage,select=-c(logwage))
summary(Wage)
inTrain=createDataPartition(y=Wage$wage,p=0.7,list=FALSE)
training=Wage[inTrain,]
testing=Wage[-inTrain,]
dim(training);dim(testing)
featurePlot(x=training[,c("age","education","jobclass")],
y=training$wage,
plot="pairs")
qplot(age,wage,data=training)
qplot(age,wage,color=jobclass,data=training)
qplot(age,wage,color=education,data=training)
modFit=train(wage ~ age + jobclass + education,
method="lm",data=training)
finMod=modFit$finalModel
print(modFit)
plot(finMod,1,pch=19,cex=0.5,col="#00000010")
qplot(finMod$fitted,finMod$residuals, color=race,data=training)
plot(finMod$residuals, pch=19)
pred=predict(modFit,testing)
qplot(wage,pred,color=year,data=testing)
modFitAll=train(wage ~ .,data=training,method="lm")
pred=predict(modFitAll,testing)
qplot(wage,pred,data=testing)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
library(caret)
library(AppliedPredictiveModeling)
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
data(iris); library(ggplot2)
names(iris)
table(iris$Species)
#Create training and test sets
inTrain=createDataPartition(y=iris$Species,p=0.7,list=FALSE)
training=iris[inTrain,]
testing=iris[-inTrain,]
dim(training);dim(testing)
library(caret)
inTrain=createDataPartition(y=iris$Species,p=0.7,list=FALSE)
training=iris[inTrain,]
testing=iris[-inTrain,]
dim(training);dim(testing)
qplot(Petal.Width,Sepal.Width,color=Species,data=training)
modFit=train(Species ~.,method="rpart",data=training)
print(modFit$finalModel)
plot(modFit$finalModel,uniform=TRUE,main="Classification Tree")
text(modFit$finalModel,use.n=TRUE,all=TRUE,cex=0.8)
library(rattle)
install.packages("rattle")
library(rattle)
fancyRpartPlot(modFit$finalModel)
install.packages("rpart")
install.packages("rpart.plot")
library(rattle)
fancyRpartPlot(modFit$finalModel)
predict(modFit,newdata=testing)
library(ElemStatLearn);data(ozone,package="ElemStatLearn")
install.packages("ElemS")
install.packages("ElemStatLearn")
library(ElemStatLearn);data(ozone,package="ElemStatLearn")
ozone=ozone[order(ozone$ozone),]
head(ozone)
library(ElemStatLearn);data(ozone,package="ElemStatLearn")
zone=ozone[order(ozone$ozone),]
head(ozone)
ozone=ozone[order(ozone$ozone),]
head(ozone)
ll=matrix(NA,nrow=10,ncol=155)
for(i in 1:10) {
#create the sample set
ss=sample(1:dim(ozone)[1],replace=1)
#create a subset of the data set based on the random sample
ozone0=ozone[ss,];
#reorder the data by the ozone variable
ozone0=ozone[order(ozone0$ozone),]
#fit a smooth loess curve to the data - similar to spline.
#span is a measure of how smooth the curve will be
loess0=loess(temperature ~ ozone,data=ozone0,span=0.2)
#predict from the loess curve what the outcome would be for all values in the dataset
ll[i,]=predict(loess0,newdata=data.frame(ozone=1:155))
}
plot(ozone$ozone,ozone$temperature, pch=19,cex=0.5)
for(i in 1:10){lines(1:155,ll[i,],col="grey",lwd=2)}
lines(1:155,apply(ll,2,mean),col="red",lwd=2)
plot(ozone$ozone,ozone$temperature, pch=19,cex=0.5)
for(i in 1:10){lines(1:155,ll[i,],col="grey",lwd=2)}
lines(1:155,apply(ll,2,mean),col="red",lwd=2)
for(i in 1:10){lines(1:155,ll[i,],col="grey",lwd=2)}
plot(ozone$ozone,ozone$temperature, pch=19,cex=0.5)
for(i in 1:10){lines(1:155,ll[i,],col="grey",lwd=2)}
View(ll)
View(ll)
rm(ll)
ll=matrix(NA,nrow=10,ncol=155)
#resample the dataset 10 times with replacement
for(i in 1:10) {
#create the sample set
ss=sample(1:dim(ozone)[1],replace=1)
#create a subset of the data set based on the random sample
ozone0=ozone[ss,];
#reorder the data by the ozone variable
ozone0=ozone[order(ozone0$ozone),]
#fit a smooth loess curve to the data - similar to spline.
#span is a measure of how smooth the curve will be
loess0=loess(temperature ~ ozone,data=ozone0,span=0.2)
#predict from the loess curve what the outcome would be for all values in the dataset
ll[i,]=predict(loess0,newdata=data.frame(ozone=1:155))
}
View(ll)
ll=matrix(NA,nrow=10,ncol=155)
#resample the dataset 10 times with replacement
for(i in 1:10) {
#create the sample set
ss=sample(1:dim(ozone)[1],replace=TRUE)
#create a subset of the data set based on the random sample
ozone0=ozone[ss,];
#reorder the data by the ozone variable
ozone0=ozone[order(ozone0$ozone),]
#fit a smooth loess curve to the data - similar to spline.
#span is a measure of how smooth the curve will be
loess0=loess(temperature ~ ozone,data=ozone0,span=0.2)
#predict from the loess curve what the outcome would be for all values in the dataset
ll[i,]=predict(loess0,newdata=data.frame(ozone=1:155))
}
View(ll)
ll=matrix(NA,nrow=10,ncol=155)
#resample the dataset 10 times with replacement
for(i in 1:10) {
#create the sample set
ss=sample(1:dim(ozone)[1],replace=T)
#create a subset of the data set based on the random sample
ozone0=ozone[ss,];
#reorder the data by the ozone variable
ozone0=ozone[order(ozone0$ozone),]
#fit a smooth loess curve to the data - similar to spline.
#span is a measure of how smooth the curve will be
loess0=loess(temperature ~ ozone,data=ozone0,span=0.2)
#predict from the loess curve what the outcome would be for all values in the dataset
ll[i,]=predict(loess0,newdata=data.frame(ozone=1:155))
}
View(ll)
ll=matrix(NA,nrow=10,ncol=155)
#resample the dataset 10 times with replacement
for(i in 1:10) {
#create the sample set
ss=sample(1:dim(ozone)[1],replace=T)
#create a subset of the data set based on the random sample
ozone0=ozone[ss,];
#reorder the data by the ozone variable
ozone0=ozone0[order(ozone0$ozone),]
#fit a smooth loess curve to the data - similar to spline.
#span is a measure of how smooth the curve will be
loess0=loess(temperature ~ ozone,data=ozone0,span=0.2)
#predict from the loess curve what the outcome would be for all values in the dataset
ll[i,]=predict(loess0,newdata=data.frame(ozone=1:155))
}
View(ll)
ll=matrix(NA,nrow=10,ncol=155)
#resample the dataset 10 times with replacement
for(i in 1:10) {
#create the sample set
ss=sample(1:dim(ozone)[1],replace=T)
#create a subset of the data set based on the random sample
ozone0=ozone[ss,];
#reorder the data by the ozone variable
ozone0=ozone0[order(ozone0$ozone),]
#fit a smooth loess curve to the data - similar to spline.
#span is a measure of how smooth the curve will be
loess0=loess(temperature ~ ozone,data=ozone0,span=0.2)
#predict from the loess curve what the outcome would be for all values in the dataset
ll[i,]=predict(loess0,newdata=data.frame(ozone=1:155))
}
plot(ozone$ozone,ozone$temperature, pch=19,cex=0.5)
for(i in 1:10){lines(1:155,ll[i,],col="grey",lwd=2)}
lines(1:155,apply(ll,2,mean),col="red",lwd=2)
predictors=data.frame(ozone=ozone$ozone)
#separate the outcomes into a vector
temperature=ozone$temperature
treebag=bag(predictors,temperature,B=10,
bagControl=bagControl(fit=ctreeBag$fit,
predict=ctreeBag$pred,
aggregate=ctreeBag$aggregate))
plot(ozone$ozone,temperature,col='lightgrey',pch=19)
points(ozone$ozone,predict(trebag$fits[[1]]$fit,predictors),pch=19,col='red')
points=ozone$ozone,predict(treebag,predictors),pch=19,col='blue')
predictors=data.frame(ozone=ozone$ozone)
temperature=ozone$temperature
treebag=bag(predictors,temperature,B=10,
bagControl=bagControl(fit=ctreeBag$fit,
predict=ctreeBag$pred,
aggregate=ctreeBag$aggregate))
install.packages("party")
treebag=bag(predictors,temperature,B=10,
bagControl=bagControl(fit=ctreeBag$fit,
predict=ctreeBag$pred,
aggregate=ctreeBag$aggregate))
plot(ozone$ozone,temperature,col='lightgrey',pch=19)
points(ozone$ozone,predict(trebag$fits[[1]]$fit,predictors),pch=19,col='red')
points(ozone$ozone,predict(treebag$fits[[1]]$fit,predictors),pch=19,col='red')
points=ozone$ozone,predict(treebag,predictors),pch=19,col='blue')
points(ozone$ozone,predict(treebag,predictors),pch=19,col='blue')
ctreeBag$fit
ctreeBag$pred
ctreeBag$aggregate
data(iris); library(ggplot2)
inTrain=createDataPartition(y=iris$Species,p=0.7,list=FALSE)
training=iris[inTrain,]
testing=iris[-inTrain,]
modFit=train(Species ~ .,data=training,method="rf",prox=TRUE)
modFit=train(Species ~ .,data=training,method="rf",prox=TRUE)
modFit
getTree(modFit$finalModel,k=2)
irisP=classCenter(training[,c(3,4)],training$Species,modFit$finalModel$prox)
irisP=as.data.frame(irisP);irisP$Species=rownames(irisP)
p=qplot(Petal.Width,Petal.Length,col=Species,data=training)
p + geom_point(aes(x=Petal.Width,y=Petal.Length,col=Species),size=5,shape=4,data=irisP)
pred=predict(modFit,testing)
testing$predRight=pred==testing$Species
table(pred,testing$Species)
qplot(Petal.Width, Petal.Length, color=predRight,data=testing,main="newdata Predictions")
library(ISLR); data(Wage);
library(ggplot2); library(caret);
Wage=subset(Wage,select=-c(logwage))
inTrain=createDataPartition(y=Wage$wage,p=0.7,list=FALSE)
training=Wage[inTrain,]; testing=Wage[-inTrain,]
modFit=trian(wage ~ ., method="gbm",data=training,verbose=FALSE)
print(modFit)
modFit=train(wage ~ ., method="gbm",data=training,verbose=FALSE)
print(modFit)
qplot(predict(modFit,testing),wage,data=testing)
data(iris);library(ggplot2)
names(iris)
table(iris$Species)
data(iris);library(ggplot2)
names(iris)
table(iris$Species)
inTrain=createDataPartition(y=iris$Species,p=0.7,list=FALSE)
training=iris[inTrain,]; testing=iris[-inTrain,]
dim(training); dim(testing)
modlda=train(Species ~ ., method="lda",data=training)
modnb=train(Species ~ ., method="nb", data=training)
plda=predict(modlda,testing)
pnb=predict(modnb,testing)
table(plda,pnb)
equalPredictions=(plda==pnb)
qplot(Petal.Width,Sepal.Width,color=equalPredictions,data=testing)
setwd("C:/Users/Tom/Documents/Coursera/Data Science/8. Practical Machine Learning/Course Project")
training=read.csv("pml-training.csv",header=TRUE)
testing=read.csv("pml-testing.csv",header=TRUE)
table(training$classe)
rm(testing)
NAcount=sapply(training,function(x) sum(is.na(x)))
goodColumns=names(NAcount[NAcount<1000])
training=training[,goodColumns]
View(training)
?is.null
Nullcount=sapply(training,function(x) sum(is.na(x)))
Nullcount
Nullcount=sapply(training,function(x) sum(is.null(x)))
Nullcount=sapply(training,function(x) sum(is.null(x)))
Nullcount
Nullcount=sapply(training, length)
Nullcount
Nullcount=sapply(training, function(x) length(x))
Nullcount
Nullcount=sapply(training, function(x) length(x)==0)
modelFit=train(classe ! ., data=training, method="glm")
modelFit=train(classe~., data=training, method="glm")
library(caret)
modelFit=train(classe~., data=training, method="glm")
summary(training)
grep("timestamp",names(training))
grep("timestamp",names(training),values=TRUE)
?grep
grep("timestamp",names(training),value=TRUE)
training= training[,-grep("timestamp",names(training),value=TRUE)]
goodColumns
grep("timestamp",goodColumns, value=TRUE)
goodColumns=goodColumns(-grep("timestamp",goodColumns, value=TRUE))
goodColumns=goodColumns[-grep("timestamp",goodColumns, value=TRUE)]
goodColumns=goodColumns[,-grep("timestamp",goodColumns, value=TRUE)]
goodColumns=goodColumns[!=grep("timestamp",goodColumns, value=TRUE)]
goodColumns=goodColumns(!=grep("timestamp",goodColumns, value=TRUE))
grep("timestamp",goodColumns, value=TRUE)
goodColumns
goodColumns(1)
goodColumns[1]
goodColumns[-"X"]
goodColumns[,-"X"]
goodColumns[-"X"]
